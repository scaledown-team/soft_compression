{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# ScaleDown: Online Soft Compression for RAG\n",
    "\n",
    "This notebook lets you test and train ScaleDown (OSCAR paper implementation) in Google Colab.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yourusername/scaledown/blob/main/ScaleDown_Colab.ipynb)\n",
    "\n",
    "## What is ScaleDown?\n",
    "\n",
    "- üöÄ **2-5√ó faster RAG inference** with minimal accuracy loss\n",
    "- üìä **16√ó compression**: Compress 128-token documents into 8 embeddings\n",
    "- üéØ **Two compressor options**: N-Layers (paper) or ModernBERT (novel)\n",
    "- üéì **Distillation-based**: Learn from teacher LLM\n",
    "\n",
    "## Colab Setup\n",
    "\n",
    "**Runtime:** Make sure you're using **GPU** runtime:\n",
    "- Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator: **T4 GPU** (free tier) or **A100** (Colab Pro)\n",
    "\n",
    "**Memory:** T4 (16GB) works for small tests. A100 (40GB) recommended for full training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## Step 1: Installation (2 minutes)\n",
    "\n",
    "Install ScaleDown and dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone"
   },
   "outputs": [],
   "source": [
    "# Clone repository (replace with your repo URL)\n",
    "!git clone https://github.com/yourusername/scaledown.git\n",
    "%cd scaledown/soft_compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": "# Install dependencies (no package installation needed!)\n!pip install -q torch transformers peft accelerate datasets tqdm bitsandbytes matplotlib\n\n# Install dataset generation dependencies\n!pip install -q sentence-transformers requests\n\nprint(\"‚úì Dependencies installed!\")\nprint(\"‚úì ScaleDown ready to use (no pip install -e . needed)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "verify"
   },
   "source": [
    "## Step 2: Quick Test (1 minute)\n",
    "\n",
    "Verify everything works before running full training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test"
   },
   "outputs": [],
   "source": "# Add path and test imports (no package installation needed!)\nimport sys\nfrom pathlib import Path\nsys.path.insert(0, '/content/scaledown/soft_compression')\n\nfrom scaledown import ScaleDownConfig, ScaleDownModel\nfrom scaledown.data import ScaleDownDataset\nfrom scaledown.training import ScaleDownTrainer\n\nprint(\"‚úì All imports successful!\")\nprint(\"‚úì ScaleDown modules loaded directly (no package installation)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_tests"
   },
   "outputs": [],
   "source": [
    "# Run automated tests (optional but recommended)\n",
    "# This will take ~2 minutes on T4 GPU\n",
    "\n",
    "!python test_training.py --compressor_type n_layers --num_examples 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "choose"
   },
   "source": "## Step 3: Choose Your Path\n\nPick one option below:\n\n- **Option A**: Quick demo with minimal data (5 minutes)\n- **Option B**: Train with real data + evaluation (30 minutes) ‚≠ê **RECOMMENDED**\n- **Option C**: Train with synthetic data (quick test)\n- **Option D**: Full OSCAR pipeline with Wikipedia-KILT (hours)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "option_a"
   },
   "source": [
    "---\n",
    "\n",
    "## Option A: Quick Demo (5 minutes)\n",
    "\n",
    "Run a minimal training loop to see ScaleDown in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "demo_data"
   },
   "outputs": [],
   "source": [
    "# Create minimal synthetic data\n",
    "demo_data = [\n",
    "    {\n",
    "        \"query\": \"What is machine learning?\",\n",
    "        \"documents\": [\n",
    "            \"Machine learning is a subset of artificial intelligence.\",\n",
    "            \"ML algorithms learn patterns from data.\",\n",
    "            \"There are supervised and unsupervised learning methods.\",\n",
    "        ],\n",
    "        \"answer\": \"Machine learning is a subset of AI that learns from data.\",\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How does photosynthesis work?\",\n",
    "        \"documents\": [\n",
    "            \"Photosynthesis converts light energy into chemical energy.\",\n",
    "            \"Plants use chlorophyll to capture sunlight.\",\n",
    "            \"The process produces glucose and oxygen.\",\n",
    "        ],\n",
    "        \"answer\": \"Photosynthesis converts light to chemical energy using chlorophyll.\",\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What is quantum computing?\",\n",
    "        \"documents\": [\n",
    "            \"Quantum computers use quantum mechanics principles.\",\n",
    "            \"They use qubits instead of classical bits.\",\n",
    "            \"Quantum computing can solve certain problems faster.\",\n",
    "        ],\n",
    "        \"answer\": \"Quantum computing uses qubits and quantum mechanics for faster computation.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "import json\n",
    "with open(\"demo_data.json\", \"w\") as f:\n",
    "    json.dump(demo_data, f, indent=2)\n",
    "\n",
    "print(f\"‚úì Created {len(demo_data)} demo examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "demo_config"
   },
   "outputs": [],
   "source": [
    "# Configure for quick demo\n",
    "from scaledown import ScaleDownConfig\n",
    "\n",
    "config = ScaleDownConfig(\n",
    "    # Use ModernBERT for faster training on free tier\n",
    "    compressor_type=\"modernbert\",\n",
    "    \n",
    "    # Small compression for demo\n",
    "    num_memory_tokens=4,\n",
    "    compression_rate=8,\n",
    "    \n",
    "    # Minimal training\n",
    "    batch_size=1,  # Small batch for T4 GPU\n",
    "    num_epochs=1,\n",
    "    max_steps=10,  # Just 10 steps\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=1,\n",
    "    \n",
    "    # Device\n",
    "    device_type=\"gpu\",\n",
    ")\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Compressor: {config.compressor_type}\")\n",
    "print(f\"  Memory tokens: {config.num_memory_tokens}\")\n",
    "print(f\"  Batch size: {config.batch_size}\")\n",
    "print(f\"  Max steps: {config.max_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "demo_train"
   },
   "outputs": [],
   "source": [
    "# Run quick training demo\n",
    "from scaledown import ScaleDownModel\n",
    "from scaledown.data import ScaleDownDataset\n",
    "from scaledown.training import ScaleDownTrainer\n",
    "\n",
    "# Load data\n",
    "dataset = ScaleDownDataset(demo_data, config)\n",
    "\n",
    "# Create model\n",
    "print(\"\\nInitializing model...\")\n",
    "model = ScaleDownModel(config)\n",
    "\n",
    "# Train\n",
    "print(\"\\nStarting training (10 steps)...\")\n",
    "trainer = ScaleDownTrainer(\n",
    "    model=model,\n",
    "    config=config,\n",
    "    train_dataset=dataset,\n",
    "    output_dir=\"./demo_output\",\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n‚úì Demo complete! Loss should decrease over steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "option_b"
   },
   "source": [
    "---\n",
    "\n",
    "## Option B: Train with Small Dataset (30 minutes)\n",
    "\n",
    "Generate 100 synthetic examples and train for real."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Option C: Train with Synthetic Data (Quick Test)\n\nGenerate synthetic examples for quick testing.",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Train with before/after evaluation and plotting\n# Adjust batch_size based on your GPU:\n# - T4 (16GB): batch_size=2\n# - A100 (40GB): batch_size=8\n\n!python train_with_evaluation.py \\\n  --train_data small_real_dataset.json \\\n  --compressor_type modernbert \\\n  --batch_size 2 \\\n  --num_epochs 1 \\\n  --output_dir /content/drive/MyDrive/scaledown_training \\\n  --logging_steps 10\n\nprint(\"\\n‚úì Training complete! Check outputs:\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Option D: Full OSCAR Pipeline with Wikipedia-KILT\n\n‚ö†Ô∏è **Warning**: This requires significant compute and storage:\n- **Wikipedia-KILT**: 35GB download\n- **Training time**: Several hours on A100\n- **Recommended**: Colab Pro with A100 or run on dedicated GPU instance",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Option B: Train with Real Data + Evaluation (30 minutes) - RECOMMENDED\n\nGet real QA data and train with automatic before/after evaluation and plots!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate_small"
   },
   "outputs": [],
   "source": [
    "# Generate 100 synthetic examples\n",
    "!python example_dataset_generation.py\n",
    "\n",
    "# This creates synthetic_train_data.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_small"
   },
   "outputs": [],
   "source": [
    "# Train with small dataset\n",
    "# Adjust batch_size based on your GPU:\n",
    "# - T4 (16GB): batch_size=2\n",
    "# - A100 (40GB): batch_size=8\n",
    "\n",
    "!python train.py \\\n",
    "  --train_data synthetic_train_data.json \\\n",
    "  --compressor_type modernbert \\\n",
    "  --batch_size 2 \\\n",
    "  --num_epochs 1 \\\n",
    "  --output_dir ./small_training_output \\\n",
    "  --logging_steps 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "option_c"
   },
   "source": [
    "---\n",
    "\n",
    "## Option C: Full Training with Real Data\n",
    "\n",
    "‚ö†Ô∏è **Warning**: This requires significant compute and storage:\n",
    "- **Wikipedia-KILT**: 35GB download\n",
    "- **Training time**: Several hours on A100\n",
    "- **Recommended**: Colab Pro with A100 or run on dedicated GPU instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_kilt"
   },
   "outputs": [],
   "source": [
    "# Download Wikipedia-KILT corpus (35GB)\n",
    "# This will take 10-30 minutes depending on connection\n",
    "\n",
    "!wget -c http://dl.fbaipublicfiles.com/KILT/kilt_knowledgesource.json\n",
    "\n",
    "print(\"‚úì Wikipedia-KILT downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate_dataset"
   },
   "outputs": [],
   "source": [
    "# Generate training dataset following OSCAR paper\n",
    "# This uses:\n",
    "# - MS MARCO queries (automatically downloaded)\n",
    "# - SPLADE-v3 retrieval\n",
    "# - Mistral-7B teacher generation\n",
    "# - DeBERTa-v3 reranking (optional)\n",
    "\n",
    "# For testing, start with small subset\n",
    "!python -m scaledown.data.prepare_dataset \\\n",
    "  --num_synthetic_queries 100 \\\n",
    "  --corpus_path kilt_knowledgesource.json \\\n",
    "  --max_corpus_size 10000 \\\n",
    "  --output_file test_data.json \\\n",
    "  --top_k_retrieval 5 \\\n",
    "  --teacher_8bit\n",
    "\n",
    "print(\"‚úì Dataset generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_full"
   },
   "outputs": [],
   "source": [
    "# Full training\n",
    "# Adjust hyperparameters based on GPU:\n",
    "# - T4: batch_size=2, num_layers=5, modernbert compressor\n",
    "# - A100: batch_size=8, num_layers=8, either compressor\n",
    "\n",
    "!python train.py \\\n",
    "  --train_data test_data.json \\\n",
    "  --compressor_type modernbert \\\n",
    "  --batch_size 4 \\\n",
    "  --num_epochs 1 \\\n",
    "  --output_dir ./scaledown_output \\\n",
    "  --logging_steps 50 \\\n",
    "  --save_steps 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inference"
   },
   "source": [
    "## Inference: Using Your Trained Model\n",
    "\n",
    "After training, use the model for RAG inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_model"
   },
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "from scaledown import ScaleDownConfig, ScaleDownModel\n",
    "import torch\n",
    "\n",
    "# Use same config as training\n",
    "config = ScaleDownConfig(\n",
    "    compressor_type=\"modernbert\",\n",
    "    device_type=\"gpu\",\n",
    ")\n",
    "\n",
    "model = ScaleDownModel(config)\n",
    "\n",
    "# Load checkpoint (adjust path)\n",
    "checkpoint_path = \"./demo_output/final/pytorch_model.bin\"\n",
    "model.load_state_dict(torch.load(checkpoint_path))\n",
    "model.eval()\n",
    "\n",
    "print(\"‚úì Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inference_example"
   },
   "outputs": [],
   "source": [
    "# Inference example\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.generator_model_name)\n",
    "\n",
    "# Your query and retrieved documents\n",
    "query = \"What is the capital of France?\"\n",
    "documents = [\n",
    "    \"Paris is the capital and largest city of France.\",\n",
    "    \"France is a country in Western Europe.\",\n",
    "    \"The Eiffel Tower is located in Paris.\",\n",
    "]\n",
    "\n",
    "# Prepare inputs (simplified - see dataset.py for full implementation)\n",
    "# In practice, you'd use ScaleDownDataset to properly format inputs\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"\\nDocuments ({len(documents)}):\")\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    print(f\"  {i}. {doc}\")\n",
    "\n",
    "print(\"\\nNote: See dataset.py for proper input formatting with memory tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "benchmark"
   },
   "source": [
    "## Performance Benchmarking\n",
    "\n",
    "Compare ScaleDown vs baseline RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "benchmark_code"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "# Measure inference time\n",
    "def benchmark_inference(model, num_trials=10):\n",
    "    \"\"\"Benchmark model inference speed.\"\"\"\n",
    "    \n",
    "    times = []\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(3):\n",
    "        # Run inference (simplified)\n",
    "        pass\n",
    "    \n",
    "    # Benchmark\n",
    "    for _ in range(num_trials):\n",
    "        start = time.time()\n",
    "        \n",
    "        # Run inference\n",
    "        with torch.no_grad():\n",
    "            # Your inference code here\n",
    "            pass\n",
    "        \n",
    "        torch.cuda.synchronize()  # Wait for GPU\n",
    "        times.append(time.time() - start)\n",
    "    \n",
    "    avg_time = sum(times) / len(times)\n",
    "    print(f\"Average inference time: {avg_time*1000:.2f} ms\")\n",
    "    print(f\"Throughput: {1/avg_time:.2f} queries/sec\")\n",
    "    \n",
    "    return avg_time\n",
    "\n",
    "# Run benchmark\n",
    "# benchmark_inference(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tips"
   },
   "source": [
    "## Colab Tips\n",
    "\n",
    "### Memory Management\n",
    "\n",
    "If you run out of memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clear_memory"
   },
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Delete models/tensors\n",
    "if 'model' in locals():\n",
    "    del model\n",
    "if 'trainer' in locals():\n",
    "    del trainer\n",
    "\n",
    "# Clear cache\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Check available memory\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "    cached = torch.cuda.memory_reserved(0) / 1e9\n",
    "    total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    \n",
    "    print(f\"GPU Memory:\")\n",
    "    print(f\"  Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"  Cached: {cached:.2f} GB\")\n",
    "    print(f\"  Total: {total:.2f} GB\")\n",
    "    print(f\"  Free: {total - allocated:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save"
   },
   "source": [
    "### Save Results to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Copy checkpoints to Drive\n",
    "!cp -r ./demo_output /content/drive/MyDrive/scaledown_checkpoints\n",
    "\n",
    "print(\"‚úì Saved to Google Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "resources"
   },
   "source": [
    "## Resources\n",
    "\n",
    "- **GitHub**: [https://github.com/yourusername/scaledown](https://github.com/yourusername/scaledown)\n",
    "- **OSCAR Paper**: [arXiv:2504.07109](https://arxiv.org/abs/2504.07109)\n",
    "- **Documentation**:\n",
    "  - [README.md](README.md) - Overview\n",
    "  - [QUICKTEST_GUIDE.md](QUICKTEST_GUIDE.md) - Quick start\n",
    "  - [DATASET_PREPARATION.md](DATASET_PREPARATION.md) - Data generation\n",
    "  - [ARCHITECTURE.md](ARCHITECTURE.md) - Technical details\n",
    "\n",
    "## Hardware Recommendations\n",
    "\n",
    "| Task | GPU | Batch Size | Training Time |\n",
    "|------|-----|------------|---------------|\n",
    "| Quick demo | T4 (free) | 1 | 5 min |\n",
    "| Small dataset (100 examples) | T4 (free) | 2 | 30 min |\n",
    "| Medium dataset (1k examples) | A100 (Pro) | 8 | 2 hours |\n",
    "| Full dataset (100k+ examples) | A100 (Pro) | 16 | 1-2 days |\n",
    "\n",
    "**Recommendation**: Use **ModernBERT compressor** on free tier (2√ó faster, less memory)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}