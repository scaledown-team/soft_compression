{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ScaleDown: 2-5√ó Faster RAG with Query-Dependent Compression\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/scaledown-team/soft_compression/blob/main/ScaleDown_Colab_v2.ipynb)\n",
        "\n",
        "**OSCAR Paper Implementation** - Train and test ScaleDown for fast RAG inference\n",
        "\n",
        "## What is ScaleDown?\n",
        "\n",
        "- üöÄ **2-5√ó faster** RAG inference with minimal accuracy loss\n",
        "- üéØ **Query-dependent** online soft compression (not offline like PISCO)\n",
        "- üìä **16√ó compression**: 128 tokens ‚Üí 8 embeddings\n",
        "- üí° **Two-stage training**: Memory-efficient approach\n",
        "\n",
        "## Runtime Setup\n",
        "\n",
        "**‚ö†Ô∏è IMPORTANT**: Change runtime to **GPU**\n",
        "- `Runtime` ‚Üí `Change runtime type` ‚Üí `Hardware accelerator: T4 GPU`\n",
        "- T4 (16GB): Works for demos and small training\n",
        "- A100 (40GB): Recommended for full training (Colab Pro)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üîß Setup (2 minutes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi --query-gpu=name,memory.total --format=csv\n",
        "\n",
        "import torch\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone repository\n",
        "!git clone https://github.com/scaledown-team/soft_compression.git\n",
        "%cd soft_compression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies (no pip install needed - research code!)\n",
        "!pip install -q torch>=2.0.0 transformers>=4.40.0 peft>=0.10.0 accelerate>=0.27.0\n",
        "!pip install -q datasets>=2.14.0 tqdm numpy matplotlib\n",
        "!pip install -q sentence-transformers requests bitsandbytes\n",
        "\n",
        "print(\"\\n‚úÖ Installation complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify imports\n",
        "import sys\n",
        "sys.path.insert(0, '/content/soft_compression')\n",
        "\n",
        "from scaledown import ScaleDownConfig, ScaleDownModel\n",
        "from scaledown.data import ScaleDownDataset\n",
        "from scaledown.training import TwoStageModernBERTTrainer, TwoStageNLayersTrainer\n",
        "\n",
        "print(\"‚úÖ All imports successful!\")\n",
        "print(\"‚úÖ Two-stage trainers available for memory-efficient training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üöÄ Quick Demo (5 minutes)\n",
        "\n",
        "Test the two-stage training with minimal data to verify everything works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create minimal demo data\n",
        "import json\n",
        "\n",
        "demo_data = [\n",
        "    {\n",
        "        \"query\": \"What is the capital of France?\",\n",
        "        \"documents\": [\n",
        "            \"Paris is the capital and largest city of France, located on the Seine River.\",\n",
        "            \"France is a country in Western Europe with several overseas regions.\",\n",
        "            \"The Eiffel Tower is an iconic landmark in Paris.\",\n",
        "        ],\n",
        "        \"answer\": \"The capital of France is Paris.\",\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"How does photosynthesis work?\",\n",
        "        \"documents\": [\n",
        "            \"Photosynthesis converts light energy into chemical energy in plants.\",\n",
        "            \"Plants use chlorophyll to capture sunlight during photosynthesis.\",\n",
        "            \"The process produces glucose and oxygen as byproducts.\",\n",
        "        ],\n",
        "        \"answer\": \"Photosynthesis converts light to chemical energy using chlorophyll, producing glucose and oxygen.\",\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What is machine learning?\",\n",
        "        \"documents\": [\n",
        "            \"Machine learning is a subset of artificial intelligence.\",\n",
        "            \"ML algorithms learn patterns from data without explicit programming.\",\n",
        "            \"Common applications include image recognition and natural language processing.\",\n",
        "        ],\n",
        "        \"answer\": \"Machine learning is a subset of AI that learns patterns from data.\",\n",
        "    },\n",
        "]\n",
        "\n",
        "with open(\"demo_data.json\", \"w\") as f:\n",
        "    json.dump(demo_data, f, indent=2)\n",
        "\n",
        "print(f\"‚úÖ Created {len(demo_data)} demo examples\")\n",
        "print(f\"   File: demo_data.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure for demo - using ModernBERT (faster on T4)\n",
        "config = ScaleDownConfig(\n",
        "    compressor_type=\"modernbert\",  # Faster, less memory\n",
        "    num_memory_tokens=4,            # Small for demo\n",
        "    compression_rate=8,\n",
        "    batch_size=1,                   # Small batch for free tier\n",
        "    num_epochs=1,\n",
        "    device_type=\"gpu\",\n",
        "    use_bf16=True,                  # Memory efficient\n",
        ")\n",
        "\n",
        "print(\"Configuration:\")\n",
        "print(f\"  Compressor: {config.compressor_type}\")\n",
        "print(f\"  Memory tokens: {config.num_memory_tokens}\")\n",
        "print(f\"  Compression: {config.compression_rate}√ó\")\n",
        "print(f\"  Batch size: {config.batch_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Two-stage training demo\n",
        "print(\"üî• Starting Two-Stage Training Demo\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create dataset\n",
        "dataset = ScaleDownDataset(demo_data, config)\n",
        "print(f\"‚úÖ Dataset created: {len(dataset)} examples\")\n",
        "\n",
        "# Create model\n",
        "print(\"\\nüì¶ Initializing model...\")\n",
        "model = ScaleDownModel(config)\n",
        "print(f\"‚úÖ Model initialized\")\n",
        "\n",
        "# Create two-stage trainer\n",
        "print(\"\\nüéØ Creating two-stage trainer...\")\n",
        "trainer = TwoStageModernBERTTrainer(\n",
        "    model=model,\n",
        "    config=config,\n",
        "    train_dataset=dataset,\n",
        "    output_dir=\"./demo_output\",\n",
        "    cache_dir=\"./demo_cache\",\n",
        ")\n",
        "print(\"‚úÖ Trainer ready\")\n",
        "\n",
        "# Run both stages\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Stage 1: Compressing documents...\")\n",
        "print(\"Stage 2: Training generator...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ Demo complete!\")\n",
        "print(\"   Model saved to: ./demo_output/final\")\n",
        "print(\"   Cache saved to: ./demo_cache\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üìä Train with Real Data (Recommended)\n",
        "\n",
        "Generate real QA data from SQuAD and train properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate 500 real examples from SQuAD\n",
        "!python prepare_small_real_dataset.py \\\n",
        "  --dataset squad \\\n",
        "  --num_examples 500 \\\n",
        "  --output small_real_dataset.json\n",
        "\n",
        "print(\"‚úÖ Real dataset generated: small_real_dataset.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Choose training approach based on GPU\n",
        "import torch\n",
        "\n",
        "gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "print(f\"GPU Memory: {gpu_memory:.1f} GB\")\n",
        "\n",
        "if gpu_memory < 20:\n",
        "    print(\"\\nüí° Recommendation: Use ModernBERT with two-stage training\")\n",
        "    compressor_type = \"modernbert\"\n",
        "    batch_size = 4 if gpu_memory > 20 else 2\n",
        "else:\n",
        "    print(\"\\nüí° Recommendation: Can use N-Layers (paper faithful)\")\n",
        "    compressor_type = \"n_layers\"\n",
        "    batch_size = 8\n",
        "\n",
        "print(f\"   Compressor: {compressor_type}\")\n",
        "print(f\"   Batch size: {batch_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train with ModernBERT (memory efficient)\n",
        "!python train_modernbert_two_stage.py \\\n",
        "  --train_data small_real_dataset.json \\\n",
        "  --cache_dir ./cache_modernbert \\\n",
        "  --output_dir ./model_modernbert \\\n",
        "  --batch_size 2 \\\n",
        "  --num_epochs 1\n",
        "\n",
        "print(\"\\n‚úÖ Training complete!\")\n",
        "print(\"   Model: ./model_modernbert/final\")\n",
        "print(\"   Cache: ./cache_modernbert\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### OR: Train with N-Layers (Paper Faithful)\n",
        "\n",
        "If you have enough memory (>20GB), try the N-Layers approach from the paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train with N-Layers (requires more memory but faithful to paper)\n",
        "!python train_nlayers_two_stage.py \\\n",
        "  --train_data small_real_dataset.json \\\n",
        "  --num_layers 8 \\\n",
        "  --cache_dir ./cache_nlayers \\\n",
        "  --output_dir ./model_nlayers \\\n",
        "  --batch_size 4 \\\n",
        "  --num_epochs 1\n",
        "\n",
        "print(\"\\n‚úÖ Training complete!\")\n",
        "print(\"   Model: ./model_nlayers/final\")\n",
        "print(\"   Cache: ./cache_nlayers\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üîç Understanding Two-Stage Training\n",
        "\n",
        "**Why two stages?**\n",
        "- **Memory efficient**: Load one model at a time\n",
        "- **Faster training**: Compression happens once, not every epoch\n",
        "- **Larger batches**: More memory available\n",
        "\n",
        "**Stage 1**: Compress all documents with compressor only\n",
        "```\n",
        "ModernBERT (0.3GB) ‚Üí Compress ‚Üí Save to disk\n",
        "```\n",
        "\n",
        "**Stage 2**: Train generator with cached embeddings\n",
        "```\n",
        "Load embeddings ‚Üí Generator (14GB) ‚Üí Train\n",
        "```\n",
        "\n",
        "**Memory savings**: ~2-4GB less peak usage!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üíæ Save to Google Drive\n",
        "\n",
        "Don't lose your trained models!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Copy model to Drive\n",
        "!mkdir -p /content/drive/MyDrive/ScaleDown\n",
        "!cp -r ./model_modernbert /content/drive/MyDrive/ScaleDown/\n",
        "\n",
        "print(\"‚úÖ Model saved to Google Drive\")\n",
        "print(\"   Location: MyDrive/ScaleDown/model_modernbert\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üßπ Cleanup & Memory Management"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clear GPU memory\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "# Delete variables\n",
        "if 'model' in locals():\n",
        "    del model\n",
        "if 'trainer' in locals():\n",
        "    del trainer\n",
        "\n",
        "# Clear cache\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Check memory\n",
        "allocated = torch.cuda.memory_allocated(0) / 1e9\n",
        "total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "print(f\"GPU Memory: {allocated:.2f} GB / {total:.1f} GB used\")\n",
        "print(f\"Free: {total - allocated:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Delete cache (frees disk space)\n",
        "# Warning: You'll need to recompute Stage 1 if you delete cache\n",
        "\n",
        "# !rm -rf ./cache_modernbert\n",
        "# !rm -rf ./cache_nlayers\n",
        "# !rm -rf ./demo_cache\n",
        "\n",
        "print(\"‚ö†Ô∏è  Uncomment above lines to delete cache\")\n",
        "print(\"   Cache allows reusing Stage 1 compression\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üìö Next Steps & Resources\n",
        "\n",
        "### Learn More\n",
        "- üìÑ [OSCAR Paper](https://arxiv.org/abs/2504.07109) - Original research\n",
        "- üìñ [README.md](README.md) - Full documentation\n",
        "- üèóÔ∏è [ARCHITECTURE.md](ARCHITECTURE.md) - Technical details\n",
        "- üî¨ [TWO_STAGE_TRAINING.md](TWO_STAGE_TRAINING.md) - Two-stage guide\n",
        "\n",
        "### Try Different Settings\n",
        "\n",
        "**Compressor types:**\n",
        "- `modernbert` - Fast, memory efficient, novel\n",
        "- `n_layers` - Paper faithful, uses first N layers\n",
        "\n",
        "**Compression rates:**\n",
        "- `compression_rate=8` - 8 tokens ‚Üí 1 embedding (faster)\n",
        "- `compression_rate=16` - 16 tokens ‚Üí 1 embedding (paper default)\n",
        "\n",
        "**Number of layers (N-Layers only):**\n",
        "- `num_layers=5` - Fastest (3.1√ó speedup)\n",
        "- `num_layers=8` - Balanced (2.4√ó speedup, paper default)\n",
        "- `num_layers=10` - Best quality\n",
        "\n",
        "### GPU Recommendations\n",
        "\n",
        "| GPU | Memory | Best Approach | Batch Size |\n",
        "|-----|--------|---------------|------------|\n",
        "| T4 | 16 GB | ModernBERT two-stage | 2 |\n",
        "| A100 | 40 GB | N-Layers two-stage | 8 |\n",
        "| A100 | 80 GB | N-Layers single-stage | 16 |\n",
        "\n",
        "### Report Issues\n",
        "\n",
        "Found a bug? [Open an issue](https://github.com/scaledown-team/soft_compression/issues)\n",
        "\n",
        "---\n",
        "\n",
        "## ‚≠ê Citation\n",
        "\n",
        "If you use ScaleDown in your research, please cite:\n",
        "\n",
        "```bibtex\n",
        "@article{louis2025oscar,\n",
        "  title={OSCAR: Online Soft Compression And Reranking},\n",
        "  author={Louis, Maxime and Formal, Thibault and Dejean, Herv√© and Clinchant, St√©phane},\n",
        "  journal={arXiv preprint arXiv:2504.07109},\n",
        "  year={2025}\n",
        "}\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
